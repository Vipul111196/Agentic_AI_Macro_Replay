"""
Test Script for Trained Graph
==============================

This script tests a trained graph on validation/test data:
1. Load trained graph from disk
2. Process validation conversations
3. Measure replay accuracy WITHOUT modifying the graph
4. Generate test report

Run: python test.py
"""

import json
import yaml
from datetime import datetime
from typing import List, Dict, Any

from src.data_parser import StreamingConversationParser
from src.image_embedder import create_embedder
from src.macro_graph import MacroGraph
from src.replay_engine import ReplayEngine
from src.replay_types import ReplayDecision
from src.utils import print_statistics, truncate_text, compare_actions


class GraphTester:
    """
    Tests a trained graph on new conversations without modifying it.
    """

    def __init__(self, graph: MacroGraph, embedder, replay_engine: ReplayEngine):
        """
        Initialize graph tester.

        Args:
            graph: Trained MacroGraph (loaded from disk)
            embedder: ImageEmbedder instance
            replay_engine: ReplayEngine instance
        """
        self.graph = graph
        self.embedder = embedder
        self.replay_engine = replay_engine

        # Test results tracking
        self.test_results = []
        self.steps_processed = 0
        self.conversations_processed = 0

        # Test embeddings tracking
        self.test_embeddings = []
        self.test_step_indices = []
    
    def test_conversation(self, conversation_id: int, messages: List[Dict]) -> List[Dict]:
        """
        Test a single conversation against the trained graph.
        
        Args:
            conversation_id: ID for this conversation
            messages: List of message dictionaries
            
        Returns:
            List of test result dictionaries for each step
        """
        parser = StreamingConversationParser()
        parser.reset(conversation_id=conversation_id)
        
        # Parse all messages
        steps = []
        for message in messages:
            step = parser.add_message(message)
            if step:
                steps.append(step)
        
        # Test each step with images
        conversation_results = []
        for step in steps:
            if not step.image_data:
                continue
            
            # Generate embedding
            image = step.get_pil_image()
            embedding = self.embedder.embed_image(image, image_data=step.image_data)

            # Store test embedding for visualization
            self.test_embeddings.append(embedding)
            self.test_step_indices.append(step.step_index)

            # Get correct action from dataset
            correct_action = step.action_text if step.action_text else "NO_ACTION"
            
            # Query replay engine (WITHOUT adding to graph)
            result = self.replay_engine.query(embedding, verbose=False)
            
            # Determine predicted action
            if result.decision == ReplayDecision.REPLAY:
                predicted_action = result.action_text if result.action_text else "NO_ACTION"
            elif result.decision == ReplayDecision.AI_FALLBACK:
                predicted_action = "AI_FALLBACK"  # Mark as needing AI
            else:  # NO_ACTION
                predicted_action = "NO_ACTION"
            
            # Check if prediction is correct with fuzzy matching for click coordinates
            is_correct = compare_actions(predicted_action, correct_action, click_tolerance=10)
            
            # For AI_FALLBACK, it's not wrong, just needs human/AI assistance
            is_ai_fallback = (result.decision == ReplayDecision.AI_FALLBACK)
            
            # Store result (including think_text for visualization)
            test_result = {
                'conversation_id': conversation_id,
                'step_index': step.step_index,
                'decision': result.decision.value,
                'predicted_action': predicted_action,
                'correct_action': correct_action,
                'confidence': result.confidence,
                'matched_node_id': result.matched_node_id,
                'is_correct': is_correct,
                'is_ai_fallback': is_ai_fallback,
                'think_text': step.think_text  # Add thinking block
            }
            
            conversation_results.append(test_result)
            self.test_results.append(test_result)
            self.steps_processed += 1
        
        self.conversations_processed += 1
        return conversation_results
    
    def get_statistics(self) -> Dict[str, Any]:
        """Calculate test statistics."""
        if not self.test_results:
            return {}
        
        total = len(self.test_results)
        
        # Decision breakdown
        replay_results = [r for r in self.test_results if r['decision'] == 'replay']
        ai_fallback_results = [r for r in self.test_results if r['decision'] == 'ai_fallback']
        no_action_results = [r for r in self.test_results if r['decision'] == 'no_action']
        
        # Accuracy metrics
        correct_predictions = sum(1 for r in self.test_results if r['is_correct'])
        
        # Replay-only accuracy (excluding AI fallback)
        replay_correct = sum(1 for r in replay_results if r['is_correct'])
        
        # Confidence stats
        confidences = [r['confidence'] for r in self.test_results if r['confidence'] is not None]
        avg_confidence = sum(confidences) / len(confidences) if confidences else 0
        
        return {
            'total_steps': total,
            'conversations_processed': self.conversations_processed,
            'correct_predictions': correct_predictions,
            'wrong_predictions': total - correct_predictions,
            'overall_accuracy': correct_predictions / total if total > 0 else 0,
            
            # Decision breakdown
            'replay_count': len(replay_results),
            'replay_correct': replay_correct,
            'replay_accuracy': replay_correct / len(replay_results) if replay_results else 0,
            
            'ai_fallback_count': len(ai_fallback_results),
            'ai_fallback_rate': len(ai_fallback_results) / total if total > 0 else 0,
            
            'no_action_count': len(no_action_results),
            
            # Confidence
            'avg_confidence': avg_confidence,
        }
    
    def save_report(self, filepath: str, test_stats: Dict[str, Any], graph_stats: Dict[str, Any]):
        """
        Save test report with all statistics and results.

        Args:
            filepath: Path to save report JSON
            test_stats: Test statistics
            graph_stats: Graph statistics
        """
        report = {
            'metadata': {
                'timestamp': datetime.now().isoformat(),
                'mode': 'TEST',
                'num_conversations': self.conversations_processed,
                'total_steps_processed': self.steps_processed,
            },
            'test_stats': test_stats,
            'graph_stats': graph_stats,
            'test_results': self.test_results,
        }

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        print(f"üìÑ Test report saved to {filepath}")

    def save_test_embeddings(self, filepath: str):
        """
        Save test embeddings for visualization.

        Args:
            filepath: Path to save embeddings (numpy format)
        """
        import numpy as np

        if not self.test_embeddings:
            print("‚ö†Ô∏è  No test embeddings to save")
            return

        test_embeddings = np.array(self.test_embeddings)
        test_step_indices = np.array(self.test_step_indices)

        # Save test embeddings
        np.savez(filepath,
                 test_embeddings=test_embeddings,
                 test_step_indices=test_step_indices)

        print(f"üé® Test embeddings saved to {filepath} ({len(test_embeddings)} embeddings)")


def main():
    """Main test pipeline."""
    print("=" * 70)
    print("üß™ AUTONOMOUS MACRO REPLAY SYSTEM - TESTING")
    print("=" * 70)
    
    # ========================================================================
    # Load Configuration
    # ========================================================================
    print("\nüìã Loading configuration from config/config.yaml...")
    with open('config/config.yaml', 'r') as f:
        config = yaml.safe_load(f)
    
    # Extract configuration values
    embedding_provider = config.get('embedding', {}).get('provider', 'voyage')
    
    # Data configuration
    TEST_FILES = config.get('data', {}).get('validation_files', ['data_validation_split/maf_validate.json'])
    MAX_CONVERSATIONS = config.get('data', {}).get('max_conversations_test', None)
    
    # Matching configuration
    SIMILARITY_THRESHOLD = config.get('matching', {}).get('similarity_threshold', 0.90)
    
    # Input/Output paths
    TRAINED_GRAPH_PATH = config.get('output', {}).get('trained_graph_path', 'models/trained_graph')
    TEST_RESULTS_DIR = config.get('output', {}).get('test_results_dir', 'test_results')
    TEST_REPORT_LATEST = config.get('output', {}).get('test_report_latest', 'test_results/test_report_latest.json')
    TEST_EMBEDDINGS_PATH = config.get('output', {}).get('test_embeddings_path', 'embeddings/test_embeddings.npz')
    
    # Cache paths
    if embedding_provider == "voyage":
        CACHE_FILE = config.get('output', {}).get('voyage_cache', 'cache/embedding_cache.pkl')
    else:
        CACHE_FILE = config.get('output', {}).get('selfmade_cache', 'cache/selfmade_embedding_cache.pkl')
    
    print(f"\n‚öôÔ∏è  Configuration:")
    print(f"  ‚Ä¢ Embedding provider: {embedding_provider.upper()}")
    print(f"  ‚Ä¢ Test files: {TEST_FILES}")
    print(f"  ‚Ä¢ Max conversations: {MAX_CONVERSATIONS if MAX_CONVERSATIONS else 'ALL'}")
    print(f"  ‚Ä¢ Trained graph: {TRAINED_GRAPH_PATH}")
    print(f"  ‚Ä¢ Similarity threshold: {SIMILARITY_THRESHOLD}")
    print(f"  ‚Ä¢ Output directory: {TEST_RESULTS_DIR}")
    print(f"  ‚Ä¢ Cache file: {CACHE_FILE}")
    
    # ========================================================================
    # STEP 1: Load Trained Graph
    # ========================================================================
    print("\nüîß Step 1: Loading trained graph...")
    
    # Initialize embedder (same as training)
    embedder = create_embedder(provider=embedding_provider, cache_file=CACHE_FILE)
    
    # Create empty graph (embedding_dim will be set from loaded graph)
    graph = MacroGraph(embedding_dim=embedder.embedding_dim, similarity_threshold=SIMILARITY_THRESHOLD)
    
    # Load trained graph from disk
    try:
        graph.load(TRAINED_GRAPH_PATH)
        print(f"‚úÖ Loaded trained graph with {len(graph.nodes)} nodes")
    except Exception as e:
        print(f"‚ùå Error loading trained graph: {e}")
        print("   Please run train.py first to create a trained graph!")
        return
    
    # Initialize replay engine with loaded graph
    replay_engine = ReplayEngine(graph, similarity_threshold=SIMILARITY_THRESHOLD)
    
    # Initialize tester
    tester = GraphTester(graph, embedder, replay_engine)
    print("‚úÖ All components initialized")
    
    # ========================================================================
    # STEP 2: Test on Validation Data
    # ========================================================================
    print("\nüß™ Step 2: Testing on validation data...")
    
    for test_file in TEST_FILES:
        print(f"\nüìÇ Loading {test_file}...")
        try:
            with open(test_file, 'r', encoding='utf-8') as f:
                conversations = json.load(f)
        except FileNotFoundError:
            print(f"‚ùå Test file not found: {test_file}")
            print("   Please ensure validation data exists!")
            continue
        
        # Limit conversations if specified
        conversations_to_test = conversations[:MAX_CONVERSATIONS] if MAX_CONVERSATIONS else conversations
        print(f"  Testing {len(conversations_to_test)} conversations...")
        
        for conv_idx, conv_obj in enumerate(conversations_to_test):
            messages = conv_obj.get('conversation', [])
            
            # Test this conversation
            results = tester.test_conversation(conv_idx, messages)
            
            # Show progress
            if conv_idx % 5 == 0 and results:
                print(f"    Conversation {conv_idx + 1}/{len(conversations_to_test)}: {len(results)} steps tested")
                
                # Show first conversation details
                if conv_idx == 0:
                    print(f"      Sample results:")
                    for result in results[:5]:  # Show first 5 steps
                        decision_emoji = "üîÑ" if result['decision'] == 'replay' else "ü§ñ" if result['decision'] == 'ai_fallback' else "‚ÑπÔ∏è"
                        correctness = "‚úÖ" if result['is_correct'] else "‚ùå" if not result['is_ai_fallback'] else "‚ö†Ô∏è"
                        
                        pred = truncate_text(result['predicted_action'], 35)
                        correct = truncate_text(result['correct_action'], 35)
                        
                        print(f"        Step {result['step_index']}: {decision_emoji} {correctness} | Pred: {pred} | True: {correct}")
    
    print(f"\n‚úÖ Tested {tester.conversations_processed} conversations with {tester.steps_processed} steps")
    
    # ========================================================================
    # STEP 3: Display Test Statistics
    # ========================================================================
    print("\n" + "=" * 70)
    print("üìä TEST RESULTS")
    print("=" * 70)
    
    # Test statistics
    test_stats = tester.get_statistics()
    if test_stats:
        print("\nüß™ Test Statistics:")
        print(f"  ‚Ä¢ Total steps tested: {test_stats['total_steps']}")
        print(f"  ‚Ä¢ Conversations tested: {test_stats['conversations_processed']}")
        print(f"  ‚Ä¢ Overall accuracy: {test_stats['overall_accuracy']:.1%}")
        print(f"  ‚Ä¢ Correct predictions: {test_stats['correct_predictions']}/{test_stats['total_steps']}")
        print(f"  ‚Ä¢ Wrong predictions: {test_stats['wrong_predictions']}")
        
        print(f"\n  üîÑ REPLAY Performance:")
        print(f"  ‚Ä¢ REPLAY count: {test_stats['replay_count']}")
        print(f"  ‚Ä¢ REPLAY correct: {test_stats['replay_correct']}")
        print(f"  ‚Ä¢ REPLAY accuracy: {test_stats['replay_accuracy']:.1%}")
        
        print(f"\n  ü§ñ AI_FALLBACK:")
        print(f"  ‚Ä¢ AI_FALLBACK count: {test_stats['ai_fallback_count']}")
        print(f"  ‚Ä¢ AI_FALLBACK rate: {test_stats['ai_fallback_rate']:.1%}")
        print(f"  ‚Ä¢ (These cases need AI/human assistance)")
        
        print(f"\n  üìà Confidence:")
        print(f"  ‚Ä¢ Average confidence: {test_stats['avg_confidence']:.4f}")
    
    # Graph statistics (should be unchanged)
    print("\nüåê Graph Statistics (from trained graph):")
    graph_stats = graph.get_statistics()
    print_statistics(graph_stats, "")
    
    # ========================================================================
    # STEP 4: Save Test Report
    # ========================================================================
    print("\nüíæ Step 3: Saving test results...")
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_file = f"{TEST_RESULTS_DIR}/test_report_{timestamp}.json"
    tester.save_report(report_file, test_stats, graph_stats)
    
    # Save latest test report
    tester.save_report(TEST_REPORT_LATEST, test_stats, graph_stats)

    # Save test embeddings for visualization
    tester.save_test_embeddings(TEST_EMBEDDINGS_PATH)

    print("\n" + "=" * 70)
    print("‚úÖ TESTING COMPLETE!")
    print("=" * 70)
    print(f"\nüìÅ Output Files:")
    print(f"  ‚Ä¢ Test Report: {report_file}")
    print(f"  ‚Ä¢ Latest: {TEST_REPORT_LATEST}")
    print(f"  ‚Ä¢ Test Embeddings: {TEST_EMBEDDINGS_PATH}")
    
    # Summary
    if test_stats:
        print(f"\nüìä Summary:")
        print(f"  ‚Ä¢ Replay Rate: {test_stats['replay_count']}/{test_stats['total_steps']} ({test_stats['replay_count']/test_stats['total_steps']:.1%})")
        print(f"  ‚Ä¢ Replay Accuracy: {test_stats['replay_accuracy']:.1%}")
        print(f"  ‚Ä¢ AI Fallback Rate: {test_stats['ai_fallback_rate']:.1%}")
    
    print("=" * 70)


if __name__ == '__main__':
    main()

